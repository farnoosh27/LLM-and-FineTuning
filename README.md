Building a GPT from scratch
[Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)

# LLM-and-FineTuning
 One good source for interview questions would be [Master Your ML & DS Interview](https://www.mlstack.cafe/blog/large-language-models-llms-interview-questions)

a useful link, explaining all the topics below can be found at [In-depth guide to fine-tuning LLMs with LoRA and QLoRA](https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora#:~:text=QLoRA%20and%20LoRA%20both%20are,of%20a%20standalone%20finetuning%20technique.)

## Adapter Layers
### Causal LLM’s, Masked LLM’s, and Seq2Seq
[Causal LLM’s, Masked LLM’s, and Seq2Seq](https://medium.com/@tom_21755/understanding-causal-llms-masked-llm-s-and-seq2seq-a-guide-to-language-model-training-d4457bbd07fa)
## PEFT
Check out the [PEFT](https://github.com/huggingface/peft) repo on the GitHub.
## LORA
## LORA + Int 8bit quantization
## QLORA
## Catastrophic forgetting
