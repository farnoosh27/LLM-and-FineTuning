Building a GPT from scratch
[Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)



# LLM-and-FineTuning
 One good source for interview questions would be [Master Your ML & DS Interview](https://www.mlstack.cafe/blog/large-language-models-llms-interview-questions)

a useful link, explaining all the topics below can be found at [In-depth guide to fine-tuning LLMs with LoRA and QLoRA](https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora#:~:text=QLoRA%20and%20LoRA%20both%20are,of%20a%20standalone%20finetuning%20technique.)

## Transformer documentation on HuggingFace
## Distributed training with ðŸ¤— Accelerate
## Adapter Layers
### Causal LLMâ€™s, Masked LLMâ€™s, and Seq2Seq
[Causal LLMâ€™s, Masked LLMâ€™s, and Seq2Seq](https://medium.com/@tom_21755/understanding-causal-llms-masked-llm-s-and-seq2seq-a-guide-to-language-model-training-d4457bbd07fa)
## PEFT
Check out the [PEFT](https://github.com/huggingface/peft) repo on the GitHub.
## how to leverage RL in finetuning
## LORA
the math behind LORA
## LORA + Int 8bit quantization
## QLORA
## Catastrophic forgetting
